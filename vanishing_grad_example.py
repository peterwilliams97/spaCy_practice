"""
    coding: utf-8

    Vanishing Gradients

    We will demonstrate the difference between using sigmoid and ReLU nonlinearities in a simple
    neural network with two hidden layers. This notebook is built off of a minimal net demo done
    by Andrej Karpathy for CS 231n, which you can check out here:
    http://cs231n.github.io/neural-networks-case-study/
"""

# Setup
import os
import numpy as np
import matplotlib.pyplot as plt


plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'


plot_dir = 'plots'
plot_n = 0
try:
    os.makedirs(plot_dir)
except FileExistsError:
    pass


def save_plot(name):
    global plot_n
    path = os.path.join(plot_dir, '%03d.%s.png' % (plot_n, name))
    print('Saving "%s"' % path)
    plt.savefig(path)
    plot_n += 1


# generate random data -- not linearly separable
np.random.seed(0)
N = 100  # number of points per class
D = 2  # dimensionality
K = 4  # number of classes
N_EPOCHS = 50000
d_theta = 2 * np.pi / K
delta = 0.9 / K
print('N=%d D=%d K=%d N_EPOCHS=%d' % (N, D, K, N_EPOCHS))
X = np.zeros((N * K, D))
num_train_examples = X.shape[0]
y = np.zeros(N * K, dtype='uint8')
for j in range(K):
    ix = range(N * j, N * (j + 1))
    r = np.sqrt(np.linspace(0.0, 1, N))  # radius
    t = np.linspace(j * d_theta, j * d_theta + 2 * np.pi, N) + np.random.randn(N) * delta  # theta
    X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]
    y[ix] = j
fig = plt.figure()
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.xlim([-1, 1])
plt.ylim([-1, 1])
plt.legend()
save_plot('input')


# The sigmoid function "squashes" inputs to lie between 0 and 1. Unfortunately, this means that for
# inputs with sigmoid output close to 0 or 1, the gradient with respect to those inputs are close to
# zero. This leads to the phenomenon of vanishing gradients, where gradients drop close to zero, and
# the net does not learn well.
#
# On the other hand, the relu function (max(0, x)) does not saturate with input size. Plot these
# functions to gain intuition.

def sigmoid(x):
    x = 1 / (1 + np.exp(-x))
    return x


def sigmoid_grad(x):
    return (x) * (1 - x)


def relu(x):
    return np.maximum(0, x)


# Let's try and see now how the two kinds of nonlinearities change deep neural net training in
# practice. Below, we build a very simple neural net with three layers (two hidden layers), for
# which you can swap out ReLU/ sigmoid nonlinearities.


def three_layer_net(NONLINEARITY, X, y, model, step_size, reg):
    """function to train a three layer neural net with either RELU or sigmoid nonlinearity via
        vanilla grad descent
    """
    # parameter initialization

    h = model['h']
    h2 = model['h2']
    W1 = model['W1']
    W2 = model['W2']
    W3 = model['W3']
    b1 = model['b1']
    b2 = model['b2']
    b3 = model['b3']

    # some hyper-parameters

    # gradient descent loop
    num_examples = X.shape[0]
    plot_array_1 = []
    plot_array_2 = []
    for i in range(N_EPOCHS):

        # FORWARD PROP

        if NONLINEARITY == 'RELU':
            hidden_layer = relu(np.dot(X, W1) + b1)
            hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)
            scores = np.dot(hidden_layer2, W3) + b3

        elif NONLINEARITY == 'SIGM':
            hidden_layer = sigmoid(np.dot(X, W1) + b1)
            hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)
            scores = np.dot(hidden_layer2, W3) + b3

        exp_scores = np.exp(scores)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # [N x K]

        #print(X.shape)
        #print(scores.shape)
        #print(np.sum(exp_scores, axis=1, keepdims=True).shape)
        #print(probs.shape)
        #assert False

        # compute the loss: average cross-entropy loss and regularization
        # v = probs[range(num_examples), y] -> 1d vector v[i] = probs[i, y[i]]]
        corect_logprobs = -np.log(probs[range(num_examples), y])
        data_loss = np.sum(corect_logprobs) / num_examples
        reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)+ 0.5*reg*np.sum(W3*W3)
        loss = data_loss + reg_loss
        if i % 1000 == 0:
            print("iteration %d: loss %f" % (i, loss))

        # compute the gradient on scores
        dscores = probs
        dscores[range(num_examples), y] -= 1
        dscores /= num_examples

        # BACKPROP HERE
        dW3 = (hidden_layer2.T).dot(dscores)
        db3 = np.sum(dscores, axis=0, keepdims=True)

        if NONLINEARITY == 'RELU':

            # backprop ReLU nonlinearity here
            dhidden2 = np.dot(dscores, W3.T)
            dhidden2[hidden_layer2 <= 0] = 0
            dW2 = np.dot( hidden_layer.T, dhidden2)
            plot_array_2.append(np.sum(np.abs(dW2)) / np.sum(np.abs(dW2.shape)))
            db2 = np.sum(dhidden2, axis=0)
            dhidden = np.dot(dhidden2, W2.T)
            dhidden[hidden_layer <= 0] = 0

        elif NONLINEARITY == 'SIGM':

            # backprop sigmoid nonlinearity here
            dhidden2 = dscores.dot(W3.T)*sigmoid_grad(hidden_layer2)
            dW2 = (hidden_layer.T).dot(dhidden2)
            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))
            db2 = np.sum(dhidden2, axis=0)
            dhidden = dhidden2.dot(W2.T)*sigmoid_grad(hidden_layer)

        dW1 =  np.dot(X.T, dhidden)
        plot_array_1.append(np.sum(np.abs(dW1))/np.sum(np.abs(dW1.shape)))
        db1 = np.sum(dhidden, axis=0)

        # add regularization
        dW3 += reg * W3
        dW2 += reg * W2
        dW1 += reg * W1

        #option to return loss, grads -- uncomment next comment
        grads={}
        grads['W1']=dW1
        grads['W2']=dW2
        grads['W3']=dW3
        grads['b1']=db1
        grads['b2']=db2
        grads['b3']=db3
        #return loss, grads

        # update
        W1 += -step_size * dW1
        b1 += -step_size * db1
        W2 += -step_size * dW2
        b2 += -step_size * db2
        W3 += -step_size * dW3
        b3 += -step_size * db3
    # evaluate training set accuracy
    if NONLINEARITY == 'RELU':
        hidden_layer = relu(np.dot(X, W1) + b1)
        hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)
    elif NONLINEARITY == 'SIGM':
        hidden_layer = sigmoid(np.dot(X, W1) + b1)
        hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)
    scores = np.dot(hidden_layer2, W3) + b3
    predicted_class = np.argmax(scores, axis=1)
    print('training accuracy: %.2f' % (np.mean(predicted_class == y)))

    # return cost, grads
    return plot_array_1, plot_array_2, W1, W2, W3, b1, b2, b3


# #### Train net with sigmoid nonlinearity first

# Initialize toy model, train sigmoid net

# N = 100  # number of points per class
# D = 2  # dimensionality
# K = 3  # number of classes
h = 50
h2 = 50
num_train_examples = X.shape[0]

model = {}
model['h'] = h  # size of hidden layer 1
model['h2'] = h2  # size of hidden layer 2
model['W1'] = 0.1 * np.random.randn(D, h)
model['b1'] = np.zeros((1, h))
model['W2'] = 0.1 * np.random.randn(h, h2)
model['b2'] = np.zeros((1, h2))
model['W3'] = 0.1 * np.random.randn(h2, K)
model['b3'] = np.zeros((1, K))

(sigm_array_1, sigm_array_2, s_W1, s_W2, s_W3, s_b1, s_b2, s_b3
    ) = three_layer_net('SIGM', X, y, model, step_size=1e-1, reg=1e-3)


# #### Now train net with ReLU nonlinearity

# In[33]:

#Re-initialize model, train relu net

model={}
model['h'] = h # size of hidden layer 1
model['h2']= h2# size of hidden layer 2
model['W1']= 0.1 * np.random.randn(D,h)
model['b1'] = np.zeros((1,h))
model['W2'] = 0.1 * np.random.randn(h,h2)
model['b2']= np.zeros((1,h2))
model['W3'] = 0.1 * np.random.randn(h2,K)
model['b3'] = np.zeros((1,K))

(relu_array_1, relu_array_2, r_W1, r_W2,r_W3, r_b1, r_b2,r_b3
    ) = three_layer_net('RELU', X, y, model, step_size=1e-1, reg=1e-3)


# # The Vanishing Gradient Issue

# We can use the sum of the magnitude of gradients for the weights between hidden layers as a cheap
# heuristic to measure speed of learning (you can also use the magnitude of gradients for each
# neuron in the hidden layer here). Intuitively, when the magnitude of the gradients of the weight
# vectors or of each neuron are large, the net is learning faster. (NOTE: For our net, each hidden
# layer has the same number of neurons. If you want to play around with this, make sure to adjust
# the heuristic to account for the number of neurons in the layer).

# In[34]:
fig = plt.figure()
plt.plot(np.array(sigm_array_1))
plt.plot(np.array(sigm_array_2))
plt.title('Sum of magnitudes of gradients -- SIGM weights')
plt.legend(("sigm first layer", "sigm second layer"))
save_plot('gradients.SIGM.weights')


# In[35]:
fig = plt.figure()
plt.plot(np.array(relu_array_1))
plt.plot(np.array(relu_array_2))
plt.title('Sum of magnitudes of gradients -- ReLU weights')
plt.legend(("relu first layer", "relu second layer"))
save_plot('gradients.ReLU.weights')


# In[36]:

# Overlaying the two plots to compare
fig = plt.figure()
plt.plot(np.array(relu_array_1))
plt.plot(np.array(relu_array_2))
plt.plot(np.array(sigm_array_1))
plt.plot(np.array(sigm_array_2))
plt.title('Sum of magnitudes of gradients -- hidden layer neurons')
plt.legend(("relu first layer", "relu second layer", "sigm first layer", "sigm second layer"))
save_plot('gradients.hidden.layer')


# #### Feel free to play around with this notebook to gain intuition. Things you might want to try:
#
# - Adding additional layers to the nets and seeing how early layers continue to train slowly for the sigmoid net
# - Experiment with hyper-parameter tuning for the nets -- changing regularization and gradient descent step size
# - Experiment with different nonlinearities -- Leaky ReLU, Maxout. How quickly do different layers learn now?
#
#

# We can see how well each classifier does in terms of distinguishing the toy data classes. As
# expected, since the ReLU net trains faster, for a set number of epochs it performs better compared
# to the sigmoid net.

# In[40]:

# plot the classifiers- SIGMOID
h = 0.02
margin = 0.2
x_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin
y_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
Z = np.dot(sigmoid(np.dot(sigmoid(np.dot(np.c_[xx.ravel(), yy.ravel()], s_W1)
                                  + s_b1), s_W2) + s_b2), s_W3) + s_b3
Z = np.argmax(Z, axis=1)
Z = Z.reshape(xx.shape)
fig = plt.figure()
plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
save_plot('classify.SIGM')


# In[38]:

# plot the classifiers-- RELU
h = 0.02
x_min, x_max = X[:, 0].min() - margin, X[:, 0].max() + margin
y_min, y_max = X[:, 1].min() - margin, X[:, 1].max() + margin
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
Z = np.dot(relu(np.dot(relu(np.dot(np.c_[xx.ravel(), yy.ravel()], r_W1)
                            + r_b1), r_W2) + r_b2), r_W3) + r_b3
Z = np.argmax(Z, axis=1)
Z = Z.reshape(xx.shape)
fig = plt.figure()
plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
save_plot('classify.ReLU')

