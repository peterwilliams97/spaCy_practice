{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<h3>Basic Recipe for Training a POS Tagger with SpaCy</h3>\n",
    "<ol>\n",
    "<li id=\"loaddatatitle\"><a href=\"#-Load-Data-\">Load Data </a>\n",
    "<ol><li>We'll be using a sample from Web Treebank corpus, in ConllX format</ol>\n",
    "<li><a href=\"#Prepare-Environment-for-New-Model\">Prepare environment for a new model</a>\n",
    "<ol><li>New model directory, with tagger and parser subdirectories. (Ensure you have permission)</ol>\n",
    "<li><a href=\"#Build-a-Vocabulary\">Build a vocabulary</a>\n",
    "\n",
    "<ol>\n",
    "<li>We are just going to load the default English Vocabulary\n",
    "<li>Defines how we get attributes (like suffix) from a token string\n",
    "<li>Includes brown cluster data on lexemes, we'll use as a feature for the parser\n",
    "</ol>\n",
    "<li> <a href=\"#Build-a-Tagger\">Build a Tagger</a>\n",
    "<ol><li>Ensure tagmap is provided if needed</ol>\n",
    "<ol><li>Which features should be used to train tagger?</ol>\n",
    "<li><a href=\"#Train-Tagger\"> Train Tagger</a>\n",
    "<ol><li>Averaged Perceptron algorithm\n",
    "<li>For each epoch: \n",
    "<ol><li>For each document in training data:\n",
    "<ol><li>For each sentence in document:\n",
    "<ol>\n",
    "    <li>Create document with sentence words (tagger not yet applied)\n",
    "    <li>Create GoldParse object with annotated labels\n",
    "    <li>Apply the tagger to the document to get predictions\n",
    "    <li>Update the tagger with GoldParse, Document (actual v predicted)\n",
    "</ol>\n",
    "</ol>\n",
    "<li> Score predictions on validation set\n",
    "</ol>\n",
    "</ol>\n",
    "<li><a href=\"#Save-Tagger\">Save Tagger</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<h3> Load Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jupyter/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll_string=11843424\n",
      "text=11843424\n",
      "Skipped 0 malformed lines\n",
      "n_sent=12543\n",
      "n_line=204605\n",
      "train=12543\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from spacy.syntax.arc_eager import PseudoProjectivity\n",
    "\n",
    "            \n",
    "def read_conllx(text):\n",
    "    bad_lines = 0\n",
    "    #t = text.strip()\n",
    "    #print(type(t), type('\\n\\n'))\n",
    "    # u = t.split(b'\\n\\n')\n",
    "    n_sent = 0\n",
    "    n_line = 0\n",
    "    print('text=%d' % len(text))\n",
    "    # text = str(text)\n",
    "    # print('text=%d' % len(text))\n",
    "    for sent in text.strip().split('\\n\\n'):\n",
    "        n_sent += 1\n",
    "        lines = sent.strip().split('\\n')\n",
    "        if lines:\n",
    "            while lines[0].startswith('#'):\n",
    "                lines.pop(0)\n",
    "            tokens = []\n",
    "            for line in lines:\n",
    "                n_line += 1\n",
    "                try:\n",
    "                    id_, word, lemma, tag, pos, morph, head, dep, _1, _2 = line.split()\n",
    "                    if '-' in id_:\n",
    "                        continue\n",
    "                    id_ = float(id_) - 1\n",
    "                    try:\n",
    "                        head = (int(head) - 1) if head != '0' else id_\n",
    "                    except:\n",
    "                        head = id_\n",
    "                    dep = 'ROOT' if dep == 'root' else dep\n",
    "                    tokens.append((id_, word, pos, int(head), dep, 'O'))\n",
    "                except:\n",
    "                    bad_lines += 1\n",
    "                    print('***', line)\n",
    "                    raise\n",
    "            if not tokens:\n",
    "                continue\n",
    "            tuples = [list(t) for t in zip(*tokens)]\n",
    "           \n",
    "            yield (None, [[tuples, []]])\n",
    "    print(\"Skipped %d malformed lines\" % bad_lines)\n",
    "    print('n_sent=%d' % n_sent)\n",
    "    print('n_line=%d' % n_line)\n",
    "\n",
    "                        \n",
    "def LoadData(url, path, make_projective=False):\n",
    "    if url:\n",
    "        conll_string = str(requests.get(url).content)\n",
    "    elif path:\n",
    "        conll_string = open(path).read()\n",
    "    print('conll_string=%d' % len(conll_string))\n",
    "    sents = list(read_conllx(conll_string))\n",
    "    if make_projective:\n",
    "        sents = PseudoProjectivity.preprocess_training_data(sents)\n",
    "    return sents\n",
    "    \n",
    "    \n",
    "train_url = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-train.conllu'\n",
    "test_url  = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English/master/en-ud-test.conllu'\n",
    "train_path = '/Users/pcadmin/code/spacy-examples/en-ud-train.conllu.txt'\n",
    "train_sents = LoadData(None, train_path)\n",
    "# test_sents = LoadData(test_url, None)\n",
    "print('train=%d' % len(train_sents))\n",
    "#print('test =%d' % len(test_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=12543\n",
      "Training corpus metadata\n",
      "\n",
      "Number of Sentences: 12543\n",
      "Number of Unique Tags: 50\n",
      "Unique Tags: ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'GW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '``']\n"
     ]
    }
   ],
   "source": [
    "def sent_iter(conll_corpus):\n",
    "    for _, doc_sents in conll_corpus:\n",
    "       # print(len(doc_sents))\n",
    "      #  print(doc_sents[0])\n",
    "        for (ids, words, tags, heads, deps, ner), _ in doc_sents:\n",
    "            yield ids, words, tags, heads, deps, ner\n",
    "            \n",
    "print('train=%d' % len(train_sents))\n",
    "sent_counter = 0\n",
    "unique_tags = set()\n",
    "for ids, words, tags, heads, deps, ner in sent_iter(train_sents):\n",
    "    unique_tags.update(tags)\n",
    "    sent_counter += 1\n",
    "doc_counter = len(train_sents)\n",
    "print(\"Training corpus metadata\")\n",
    "print()\n",
    "print(\"Number of Sentences: %d\" % sent_counter)\n",
    "print(\"Number of Unique Tags: %d\" % len(unique_tags))\n",
    "print(\"Unique Tags: %s\" % sorted(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n",
    "<br>\n",
    "### Prepare Environment for New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy.en' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1fdc4c5d534e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtagger_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'en-1.1.0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtagger_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'custom-pos-tagger'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'spacy.en' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "def prepare_environment_for_new_tagger(model_path, tagger_path):\n",
    "    if not model_dir.exists():\n",
    "        model_dir.mkdir()\n",
    "    if not tagger_path.exists():\n",
    "        tagger_path.mkdir()\n",
    "        \n",
    "data_dir = spacy.en.get_data_path()\n",
    "model_dir = data_dir / 'en-1.1.0'\n",
    "tagger_dir = model_dir / 'custom-pos-tagger'\n",
    "prepare_environment_for_new_tagger(model_dir, tagger_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n",
    "<br>\n",
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "def build_vocab(model_dir, vec_path = None, lexeme_path = None):\n",
    "    vocab = Vocab.load(model_dir)\n",
    "    if lexeme_path:\n",
    "        vocab.load_lexemes(lexeme_path)\n",
    "    if vec_path:\n",
    "        vocab.load_vectors_from_bin_loc(vec_path)\n",
    "        \n",
    "    return vocab\n",
    "    \n",
    "lexeme_path = model_dir / 'vocab' / 'lexemes.bin'\n",
    "vocab = build_vocab(model_dir, lexeme_path=lexeme_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Value for 'He': 126\n"
     ]
    }
   ],
   "source": [
    "#test clusters are available\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "doc = Doc(vocab, words=[u'He',u'ate',u'pizza',u'.'])\n",
    "print \"Cluster Value for '{}': {}\".format(*[doc[0], doc[0].cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n",
    "<br>\n",
    "### Build a Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.tagger import Tagger\n",
    "from spacy.tagger import *\n",
    "\n",
    "features = [\n",
    "    (W_orth,),(W_shape,),(W_cluster,),(W_flags,),(W_suffix,),(W_prefix,),    #current word attributes   \n",
    "    (P1_pos,),(P1_cluster,),(P1_flags,),(P1_suffix,),                        #-1 word attributes \n",
    "    (P2_pos,),(P2_cluster,),(P2_flags,),                                     #-2 word attributes  \n",
    "    (N1_orth,),(N1_suffix,),(N1_cluster,),(N1_flags,),                       #+1 word attributes       \n",
    "    (N2_orth,),(N2_cluster,),(N2_flags,),                                    #+2 word attributes \n",
    "    (P1_lemma, P1_pos),(P2_lemma, P2_pos), (P1_pos, P2_pos),(P1_pos, W_orth) #combination attributes \n",
    "]\n",
    "\n",
    "features = spacy.en.English.Defaults.tagger_features\n",
    "tag_map = spacy.en.tag_map\n",
    "statistical_model = spacy.tagger.TaggerModel(features)\n",
    "tagger = Tagger(vocab, tag_map=tag_map, statistical_model = statistical_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n",
    "<br>\n",
    "### Train Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_datascience": {},
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:\t\tPOS Tag Accuracy\n",
      "Pretraining:\t\t0.000\n",
      "0:\t\t\t87.655\n",
      "1:\t\t\t89.122\n",
      "2:\t\t\t91.250\n",
      "3:\t\t\t91.110\n",
      "4:\t\t\t91.453\n",
      "5:\t\t\t91.851\n",
      "6:\t\t\t92.545\n",
      "7:\t\t\t92.302\n",
      "8:\t\t\t92.246\n",
      "9:\t\t\t91.843\n"
     ]
    }
   ],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "import random\n",
    "\n",
    "\n",
    "def score_model(vocab, tagger, gold_docs, verbose=False):\n",
    "    scorer = Scorer()\n",
    "    for _, gold_doc in gold_docs:\n",
    "        for (ids, words, tags, heads, deps, entities), _ in gold_doc:\n",
    "            doc = Doc(vocab, words=map(unicode,words))\n",
    "            tagger(doc)\n",
    "            gold = GoldParse(doc, tags=tags)\n",
    "            scorer.score(doc, gold, verbose=verbose)\n",
    "    return scorer  \n",
    "\n",
    "\n",
    "def train(tagger, vocab, train_sents, test_sents, model_dir, n_iter=20, seed = 0, feat_set = u'basic'):\n",
    "    scorer = score_model(vocab, tagger, test_sents)\n",
    "    print('%s:\\t\\t%s' % (\"Iteration\", \"POS Tag Accuracy\"))            \n",
    "    print('%s:\\t\\t%.3f' % (\"Pretraining\", scorer.tags_acc))        \n",
    "    \n",
    "    #TRAINING STARTS HERE\n",
    "    for itn in range(n_iter):\n",
    "        for ids, words, tags, heads, deps, ner in sent_iter(train_sents):\n",
    "            doc = Doc(vocab, words=map(unicode,words))\n",
    "            gold = GoldParse(doc, tags=tags, heads=heads, deps=deps)\n",
    "            tagger(doc)\n",
    "            tagger.update(doc, gold)\n",
    "        random.shuffle(train_sents)\n",
    "        scorer = score_model(vocab, tagger, test_sents)\n",
    "        print('%d:\\t\\t\\t%.3f' % (itn, scorer.tags_acc))\n",
    "    return tagger\n",
    "trained_tagger = train(tagger, vocab, train_sents, test_sents, model_dir, n_iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "<a href=\"#loaddatatitle\">back</a>\n",
    "<br>\n",
    "### Save Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_dir(path):\n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "        \n",
    "ensure_dir(tagger_dir)\n",
    "trained_tagger.model.dump(str(tagger_dir / 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Notes\n",
    "<br>\n",
    "1. Spacy will be rolling out a neural network model soon!\n",
    "<br>\n",
    "<br>\n",
    "2. Checkout Speech and Language Processing by Daniel Jurafsky and James H. Martin\n",
    "<br>\n",
    "<br>\n",
    "3. Next section: Vector space models for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_datascience": {
   "notebookId": 750
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
